<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Introduction on Animal Wikipages</title><link>https://natasha0301.github.io/Project-website/</link><description>Recent content in Introduction on Animal Wikipages</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://natasha0301.github.io/Project-website/index.xml" rel="self" type="application/rss+xml"/><item><title>Data description</title><link>https://natasha0301.github.io/Project-website/data-description/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://natasha0301.github.io/Project-website/data-description/</guid><description>The data consists of</description></item><item><title>Network analysis</title><link>https://natasha0301.github.io/Project-website/network-analysis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://natasha0301.github.io/Project-website/network-analysis/</guid><description>Lorem ipsum dolor sit amet, consectetur adipiscing elit. In nulla tellus, tempus sed lobortis quis, venenatis ac ante. Maecenas accumsan augue ultricies metus hendrerit, in ultrices urna fringilla. Suspendisse lobortis egestas magna, sit amet fermentum ligula tincidunt vitae. Suspendisse cursus non dui a vulputate. Cras vestibulum vulputate enim eu placerat. Ut scelerisque semper justo sit amet auctor. Aliquam sit amet iaculis tortor.
Nulla in justo hendrerit, tincidunt mauris et, porta est.</description></item><item><title>Text analysis</title><link>https://natasha0301.github.io/Project-website/text-analysis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://natasha0301.github.io/Project-website/text-analysis/</guid><description>We decided to do text analysis on the reptile database subset as described in earlier section. We chose to do it only on the reptiles due to the nature of scraping that many pages takes quite a while, but also more importantly due to the size of the resulting dataset being quite quite large. The text analysis consisted of scraping all the wikipedia pages mentioned and doign TF-IDF on all the body text of the page.</description></item></channel></rss>